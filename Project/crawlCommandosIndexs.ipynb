{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audiblemaple/Data-acquisition-course/blob/main/Project/crawlCommandosIndexs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu-nszRXeyxQ",
        "outputId": "7ca7c1ea-fbd4-442b-ca09-10cde50c32de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVAU89ONSyXz"
      },
      "source": [
        "Code from HW2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDagiXcExRfg"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "print(\"Downloading NLTK resources...\")\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "print(\"NLTK resources downloaded.\\n\")\n",
        "\n",
        "# Function to extract and normalize links from a webpage\n",
        "def get_links(url):\n",
        "    print(f\"Extracting links from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
        "        # Normalize links to absolute URLs\n",
        "        normalized_links = []\n",
        "        for link in links:\n",
        "            if link.startswith('/'):\n",
        "                # Convert relative URL to absolute\n",
        "                normalized_link = urljoin(url, link)\n",
        "                normalized_links.append(normalized_link)\n",
        "            elif link.startswith('http'):\n",
        "                normalized_links.append(link)\n",
        "            # Ignore other types of links (e.g., mailto:, javascript:)\n",
        "        print(f\"Found {len(normalized_links)} normalized links.\\n\")\n",
        "        return normalized_links\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting links from {url}: {e}\\n\")\n",
        "        return []\n",
        "\n",
        "# Text preprocessing functions\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stop_words(text, isAlphaChecking=True):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if isAlphaChecking:\n",
        "        words = [word for word in text if word.lower() not in stop_words and word.isalpha()]\n",
        "    else:\n",
        "        words = [word for word in text if word.lower() not in stop_words]\n",
        "    return words\n",
        "\n",
        "def trim_words(words):\n",
        "    cleaned_words = [word.rstrip(\",.\\\\/?!'\\\"\") for word in words]\n",
        "    return cleaned_words\n",
        "\n",
        "def apply_stemming(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def apply_lemmatization(words):\n",
        "    print(\"CHECK!\")\n",
        "    print(words[:20])\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    print(lemmatized_words[:20])\n",
        "    return lemmatized_words\n",
        "\n",
        "# Function to create index and collect search results\n",
        "def create_index(url, max_pages=250):\n",
        "    print(\"Starting to create index and collect search results...\\n\")\n",
        "    index = defaultdict(dict)  # word -> {doc_id: term_frequency}\n",
        "    visited = set()\n",
        "    queue = [url]\n",
        "    results = []  # To store search results with relevant fields\n",
        "    pages_crawled = 0\n",
        "    link_ids = {}  # link -> unique ID\n",
        "\n",
        "    while queue and pages_crawled < max_pages:\n",
        "        current_url = queue.pop(0)\n",
        "\n",
        "        # Normalize the URL to avoid duplicates (e.g., remove trailing slash)\n",
        "        parsed_url = urlparse(current_url)\n",
        "        normalized_url = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path.rstrip('/')\n",
        "\n",
        "        if normalized_url in visited:\n",
        "            print(f\"Already visited: {normalized_url}\\n\")\n",
        "            continue\n",
        "\n",
        "        visited.add(normalized_url)\n",
        "        pages_crawled += 1\n",
        "        print(f\"Crawling ({pages_crawled}/{max_pages}): {normalized_url}\")\n",
        "\n",
        "        try:\n",
        "            session = requests.Session()\n",
        "            session.max_redirects = 2\n",
        "            response = session.get(normalized_url, allow_redirects=True, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract title\n",
        "            title = soup.title.string.strip() if soup.title else 'No Title'\n",
        "\n",
        "            # Extract full text\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            snippet = text[:200] + '...' if len(text) > 200 else text\n",
        "\n",
        "            # Append to results\n",
        "            results.append({\n",
        "                'URL': normalized_url,\n",
        "                'Title': title,\n",
        "                'Snippet': snippet,\n",
        "                'FullText': text  # Store full text for TF-IDF\n",
        "            })\n",
        "\n",
        "            print(f\"Title Extracted: {title}\")\n",
        "            print(f\"Snippet Extracted: {snippet}\\n\")\n",
        "\n",
        "            # Process words for indexing\n",
        "            words = remove_stop_words(text.split())\n",
        "            words = trim_words(words)\n",
        "            words = apply_stemming(words)  # or apply_lemmatization(words) based on preference\n",
        "\n",
        "            print(f\"Number of words after preprocessing: {len(words)}\")\n",
        "\n",
        "            for word in words:\n",
        "                # Assign a unique ID to each document\n",
        "                if normalized_url not in link_ids:\n",
        "                    link_ids[normalized_url] = pages_crawled  # Assigning ID based on crawl order\n",
        "                doc_id = link_ids[normalized_url]\n",
        "\n",
        "                # Update term frequency for the word in the document\n",
        "                if doc_id in index[word]:\n",
        "                    index[word][doc_id] += 1\n",
        "                else:\n",
        "                    index[word][doc_id] = 1\n",
        "\n",
        "            print(f\"Indexed {len(words)} words for document ID {doc_id}.\\n\")\n",
        "\n",
        "            # Get and enqueue links\n",
        "            links = get_links(normalized_url)\n",
        "            for link in links:\n",
        "                # Only enqueue links that start with the base URL and haven't been visited\n",
        "                if link.startswith(url) and link not in visited and 'services/certifications' not in link:\n",
        "                    queue.append(link)\n",
        "            print(f\"Enqueued {len(links)} new links.\\n\")\n",
        "        except requests.Timeout:\n",
        "            print(f\"Timeout: {normalized_url}\\n\")\n",
        "            continue\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"RequestException: {e} for URL: {normalized_url}\\n\")\n",
        "            continue\n",
        "\n",
        "    # Create reverse mapping from doc_id to URL\n",
        "    doc_id_to_url = {v: k for k, v in link_ids.items()}\n",
        "\n",
        "    print(f\"Finished crawling. Total pages crawled: {pages_crawled}\")\n",
        "    print(f\"Total unique words indexed: {len(index)}\\n\")\n",
        "    return index, results, doc_id_to_url\n",
        "\n",
        "# Function to rank words based on total frequency across all documents\n",
        "def create_ranked_words(index):\n",
        "    print(\"Ranking words based on total frequency across all documents...\")\n",
        "    sorted_dict = {}\n",
        "    for word, doc_dict in index.items():\n",
        "        total_count = sum(doc_dict.values())\n",
        "        sorted_dict[word] = total_count\n",
        "    # Sort the dictionary by counts in descending order\n",
        "    sorted_dict = dict(sorted(sorted_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "    ranked_dict = {}\n",
        "    rank = 1\n",
        "    for word, counter in sorted_dict.items():\n",
        "        ranked_dict[word] = {'rank': rank, 'counter': counter}\n",
        "        rank += 1\n",
        "    print(\"Word ranking completed.\\n\")\n",
        "    return ranked_dict\n",
        "\n",
        "# Function to export search results to Excel\n",
        "def export_search_results(search_results, filename='search_results.xlsx'):\n",
        "    print(f\"Exporting search results to {filename}...\")\n",
        "    df = pd.DataFrame(search_results)\n",
        "    try:\n",
        "      df.to_excel(filename, index=False)\n",
        "      print(f\"Search results have been exported to {filename}\\n\")\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "# Function to export inverted index to Excel\n",
        "def export_inverted_index(index, chosen_words, filename='inverted_index.xlsx'):\n",
        "    print(f\"Building inverted index for the top {len(chosen_words)} words and exporting to {filename}...\")\n",
        "    inverted_index_data = []\n",
        "    for word in chosen_words:\n",
        "        doc_dict = index[word]\n",
        "        # Limit to first 20 documents if necessary\n",
        "        limited_docs = dict(sorted(doc_dict.items()))\n",
        "        for doc_id, count in limited_docs.items():\n",
        "            inverted_index_data.append({'Word': word, 'Document_ID': doc_id, 'Term_Frequency': count})\n",
        "    df_inverted = pd.DataFrame(inverted_index_data)\n",
        "    df_inverted.to_excel(filename, index=False)\n",
        "    print(f\"Inverted index has been exported to {filename}\\n\")\n",
        "\n",
        "def get_words_per_url(doc_id_to_url):\n",
        "    updated_doc_id_to_url = {}\n",
        "    for doc_id, url in doc_id_to_url.items():\n",
        "      try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            text = soup.get_text()\n",
        "            word_count = len(text.split())\n",
        "      except Exception as e:\n",
        "            print(f\"Error accessing {url}: {e}\")\n",
        "            word_count = 0\n",
        "      updated_doc_id_to_url[doc_id] = {\"url\": url, \"totalwords\": word_count}\n",
        "    return updated_doc_id_to_url\n",
        "\n",
        "def create_term_doc_excel_for_all_terms(index, doc_id_to_url, file_path=\"term_doc_appearance_all_terms.xlsx\"):\n",
        "    # Extract all terms from the index\n",
        "    all_terms = sorted(index.keys())\n",
        "\n",
        "    # Extract all unique document IDs for consistent column ordering\n",
        "    all_doc_ids = sorted({doc_id for term_docs in index.values() for doc_id in term_docs})\n",
        "\n",
        "    # Map document IDs to URLs for column headers\n",
        "    urls = [doc_id_to_url.get(doc_id, f\"Doc {doc_id}\") for doc_id in all_doc_ids]\n",
        "\n",
        "    # Prepare data for the DataFrame\n",
        "    data = {\"term/doc\": []}\n",
        "    for url in urls:\n",
        "        data[url] = []\n",
        "\n",
        "    # Populate term frequencies for each term\n",
        "    for term in all_terms:\n",
        "        data[\"term/doc\"].append(term)\n",
        "        for doc_id in all_doc_ids:\n",
        "            # Get the frequency of the term in the document, or 0 if not present\n",
        "            data[doc_id_to_url.get(doc_id, f\"Doc {doc_id}\")].append(index[term].get(doc_id, 0))\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to Excel\n",
        "    df.to_excel(file_path, index=False)\n",
        "    return\n",
        "\n",
        "def create_term_doc_excel_with_query(index, doc_id_to_url, query, file_path=\"term_doc_appearance.xlsx\"):\n",
        "    # Preprocess the query: remove stop words, apply stemming\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize query, remove stop words, and apply stemming\n",
        "    query_terms = query.split()\n",
        "    processed_query_terms = [\n",
        "        stemmer.stem(word.lower())\n",
        "        for word in query_terms\n",
        "        if word.lower() not in stop_words and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Extract relevant terms (only those in the index)\n",
        "    relevant_terms = [term for term in processed_query_terms if term in index]\n",
        "\n",
        "    # Extract all unique document IDs for consistent column ordering\n",
        "    all_doc_ids = sorted({doc_id for term_docs in index.values() for doc_id in term_docs})\n",
        "\n",
        "    updated_doc_id_to_url = get_words_per_url(doc_id_to_url)\n",
        "    # print(len(updated_doc_id_to_url))\n",
        "    # print(updated_doc_id_to_url.get(1))\n",
        "    # Map document IDs to URLs for column headers\n",
        "    urls = [doc_id_to_url.get(doc_id, f\"Doc {doc_id}\") for doc_id in all_doc_ids]\n",
        "    # print(urls)\n",
        "    # Prepare data for the DataFrame\n",
        "    data = {\"term/doc\": []}\n",
        "    for url in urls:\n",
        "        data[url] = []\n",
        "\n",
        "    # Populate term frequencies for each relevant term\n",
        "    for term in relevant_terms:\n",
        "        data[\"term/doc\"].append(term)\n",
        "        for doc_id in all_doc_ids:\n",
        "            data[doc_id_to_url.get(doc_id, f\"Doc {doc_id}\")].append(index[term].get(doc_id, 0))\n",
        "            # print(data)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save DataFrame to Excel\n",
        "    df.to_excel(file_path, index=False)\n",
        "    return\n",
        "\n",
        "# Main Execution\n",
        "\n",
        "# TODO:\n",
        "#   remove 16 word limit for inverted index and calculate for all documents instead of 20\n",
        "#   after retrieving the results and calculating the inverted index run each query against the results to get the search results.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    website_url = 'https://www.who.int'\n",
        "    query = \"'which medicine is used for covid 19'\"\n",
        "    # ============================\n",
        "    # 1. Creating the Index and Collecting Results\n",
        "    # ============================\n",
        "\n",
        "    max_pages_to_crawl = 30\n",
        "    index, search_results, doc_id_to_url = create_index(website_url, max_pages=max_pages_to_crawl)\n",
        "\n",
        "    # Create ranked words\n",
        "    ranked_words = create_ranked_words(index)\n",
        "    chosen_words = list(ranked_words.keys())#[:15]   extract top 15 keywords\n",
        "    print(f\"Chosen Words for Inverted Index: {chosen_words}\\n\")\n",
        "\n",
        "    # ============================\n",
        "    # 2. Export Search Results to Excel\n",
        "    # ============================\n",
        "\n",
        "    export_search_results(search_results, filename='search_results.xlsx')\n",
        "\n",
        "    # ============================\n",
        "    # 3. Build and Export Inverted Index\n",
        "    # ============================\n",
        "    export_inverted_index(index, chosen_words, filename='inverted_index.xlsx')\n",
        "    create_term_doc_excel_with_query(index, doc_id_to_url, query)\n",
        "    create_term_doc_excel_for_all_terms(index, doc_id_to_url)\n",
        "    end = time.time()\n",
        "    print(end - start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn7s2_y3SuRb"
      },
      "source": [
        "crawling and indexing without Excel files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPGEqlmCI06I",
        "outputId": "9cb5fce0-7d2c-42a3-dfed-b6b9b70d0ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK resources...\n",
            "NLTK resources downloaded.\n",
            "\n",
            "Crawling (1/20): https://www.who.int\n",
            "Title Extracted: World Health Organization (WHO)\n",
            "Snippet Extracted: World Health Organization (WHO) Skip to main content Global Regions WHO Regional websites Africa Ame\n",
            "\n",
            "Number of tokens after preprocessing: 440\n",
            "Indexed 440 tokens for document ID 1.\n",
            "\n",
            "Extracting links from: https://www.who.int\n",
            "Found 237 normalized links.\n",
            "\n",
            "Crawling (2/20): https://www.who.int/westernpacific\n",
            "Title Extracted: WHO Western Pacific | World Health Organization\n",
            "Snippet Extracted: WHO Western Pacific | World Health Organization Skip to main content Global Regions WHO Regional web\n",
            "\n",
            "Number of tokens after preprocessing: 477\n",
            "Indexed 477 tokens for document ID 2.\n",
            "\n",
            "Extracting links from: https://www.who.int/westernpacific\n",
            "Found 212 normalized links.\n",
            "\n",
            "Crawling (3/20): https://www.who.int/countries\n",
            "Title Extracted: Countries | World Health Organization\n",
            "Snippet Extracted: Countries | World Health Organization Skip to main content Global Regions WHO Regional websites Afri\n",
            "\n",
            "Number of tokens after preprocessing: 350\n",
            "Indexed 350 tokens for document ID 3.\n",
            "\n",
            "Extracting links from: https://www.who.int/countries\n",
            "Found 212 normalized links.\n",
            "\n",
            "Crawling (4/20): https://www.who.int/about\n",
            "Title Extracted: About WHO\n",
            "Snippet Extracted: About WHO Skip to main content Global Regions WHO Regional websites Africa Americas South-East Asia \n",
            "\n",
            "Number of tokens after preprocessing: 481\n",
            "Indexed 481 tokens for document ID 4.\n",
            "\n",
            "Extracting links from: https://www.who.int/about\n",
            "Found 228 normalized links.\n",
            "\n",
            "Crawling (5/20): https://www.who.int/news/item/23-01-2025-georgia-certified-malaria-free-by-who\n",
            "Title Extracted: Georgia certified malaria-free by WHO\n",
            "Snippet Extracted: Georgia certified malaria-free by WHO Skip to main content Global Regions WHO Regional websites Afri\n",
            "\n",
            "Number of tokens after preprocessing: 596\n",
            "Indexed 596 tokens for document ID 5.\n",
            "\n",
            "Extracting links from: https://www.who.int/news/item/23-01-2025-georgia-certified-malaria-free-by-who\n",
            "Found 220 normalized links.\n",
            "\n",
            "Crawling (6/20): https://www.who.int/news/item/21-01-2025-who-comments-on-united-states--announcement-of-intent-to-withdraw\n",
            "Title Extracted: WHO comments on United States’ announcement of intent to withdraw\n",
            "Snippet Extracted: WHO comments on United States’ announcement of intent to withdraw Skip to main content Global Region\n",
            "\n",
            "Number of tokens after preprocessing: 441\n",
            "Indexed 441 tokens for document ID 6.\n",
            "\n",
            "Extracting links from: https://www.who.int/news/item/21-01-2025-who-comments-on-united-states--announcement-of-intent-to-withdraw\n",
            "Found 216 normalized links.\n",
            "\n",
            "Crawling (7/20): https://www.who.int/news/item/19-01-2025-the-ceasefire-in-gaza-brings-hope--but-immense-challenges-lie-ahead-to-restore-the-health-system\n",
            "Title Extracted: The ceasefire in Gaza brings hope, but immense challenges lie ahead to restore the health system\n",
            "Snippet Extracted: The ceasefire in Gaza brings hope, but immense challenges lie ahead to restore the health system Ski\n",
            "\n",
            "Number of tokens after preprocessing: 676\n",
            "Indexed 676 tokens for document ID 7.\n",
            "\n",
            "Extracting links from: https://www.who.int/news/item/19-01-2025-the-ceasefire-in-gaza-brings-hope--but-immense-challenges-lie-ahead-to-restore-the-health-system\n",
            "Found 216 normalized links.\n",
            "\n",
            "Crawling (8/20): https://www.who.int/news/item/16-01-2025-who-launches-us-1.5-billion-health-emergency-appeal-to-tackle-unprecedented-global-health-crises\n",
            "Title Extracted: WHO launches US$ 1.5 billion Health Emergency Appeal to tackle unprecedented global health crises\n",
            "Snippet Extracted: WHO launches US$ 1.5 billion Health Emergency Appeal to tackle unprecedented global health crises Sk\n",
            "\n",
            "Number of tokens after preprocessing: 584\n",
            "Indexed 584 tokens for document ID 8.\n",
            "\n",
            "Extracting links from: https://www.who.int/news/item/16-01-2025-who-launches-us-1.5-billion-health-emergency-appeal-to-tackle-unprecedented-global-health-crises\n",
            "Found 220 normalized links.\n",
            "\n",
            "Crawling (9/20): https://www.who.int/emergencies/overview\n",
            "Title Extracted: World Health Organization Emergencies Programme\n",
            "Snippet Extracted: World Health Organization Emergencies Programme Skip to main content Global Regions WHO Regional web\n",
            "\n",
            "Number of tokens after preprocessing: 629\n",
            "Indexed 629 tokens for document ID 9.\n",
            "\n",
            "Extracting links from: https://www.who.int/emergencies/overview\n",
            "Found 280 normalized links.\n",
            "\n",
            "Crawling (10/20): https://www.who.int/director-general/speeches/detail/who-director-general-s-opening-remarks-at-the-media-briefing-on-outbreak-of-marburg-virus-disease---20-january-2025\n",
            "Title Extracted: WHO Director-General's opening remarks at the media briefing on outbreak of Marburg virus disease – 20 January 2025\n",
            "Snippet Extracted: WHO Director-General's opening remarks at the media briefing on outbreak of Marburg virus disease – \n",
            "\n",
            "Number of tokens after preprocessing: 627\n",
            "Indexed 627 tokens for document ID 10.\n",
            "\n",
            "Extracting links from: https://www.who.int/director-general/speeches/detail/who-director-general-s-opening-remarks-at-the-media-briefing-on-outbreak-of-marburg-virus-disease---20-january-2025\n",
            "Found 220 normalized links.\n",
            "\n",
            "Crawling (11/20): https://www.who.int/emergencies/situations/mpox-outbreak\n",
            "Title Extracted: Mpox outbreak\n",
            "Snippet Extracted: Mpox outbreak Skip to main content Global Regions WHO Regional websites Africa Americas South-East A\n",
            "\n",
            "Number of tokens after preprocessing: 603\n",
            "Indexed 603 tokens for document ID 11.\n",
            "\n",
            "Extracting links from: https://www.who.int/emergencies/situations/mpox-outbreak\n",
            "Found 260 normalized links.\n",
            "\n",
            "Crawling (12/20): https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports\n",
            "Title Extracted: Coronavirus Disease (COVID-19) Situation Reports\n",
            "Snippet Extracted: Coronavirus Disease (COVID-19) Situation Reports Skip to main content Global Regions WHO Regional we\n",
            "\n",
            "Number of tokens after preprocessing: 1380\n",
            "Indexed 1380 tokens for document ID 12.\n",
            "\n",
            "Extracting links from: https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports\n",
            "Found 441 normalized links.\n",
            "\n",
            "Crawling (13/20): https://www.who.int/about/funding/invest-in-who\n",
            "Title Extracted: Invest in WHO\n",
            "Snippet Extracted: Invest in WHO Skip to main content Global Regions WHO Regional websites Africa Americas South-East A\n",
            "\n",
            "Number of tokens after preprocessing: 586\n",
            "Indexed 586 tokens for document ID 13.\n",
            "\n",
            "Extracting links from: https://www.who.int/about/funding/invest-in-who\n",
            "Found 241 normalized links.\n",
            "\n",
            "Crawling (14/20): https://www.who.int/podcasts/episode/science-in-5/episode--133---measles--a-growing-threat\n",
            "Title Extracted: Episode #133 - Measles: a growing threat\n",
            "Snippet Extracted: Episode #133 - Measles: a growing threat Skip to main content World Health Organization Global Regio\n",
            "\n",
            "Number of tokens after preprocessing: 597\n",
            "Indexed 597 tokens for document ID 14.\n",
            "\n",
            "Extracting links from: https://www.who.int/podcasts/episode/science-in-5/episode--133---measles--a-growing-threat\n",
            "Found 217 normalized links.\n",
            "\n",
            "Crawling (15/20): https://www.who.int/publications\n",
            "Title Extracted: Publications\n",
            "Snippet Extracted: Publications Skip to main content Global Regions WHO Regional websites Africa Americas South-East As\n",
            "\n",
            "Number of tokens after preprocessing: 412\n",
            "Indexed 412 tokens for document ID 15.\n",
            "\n",
            "Extracting links from: https://www.who.int/publications\n",
            "Found 233 normalized links.\n",
            "\n",
            "Crawling (16/20): https://www.who.int/director-general/speeches\n",
            "Title Extracted: Speeches\n",
            "Snippet Extracted: Speeches Skip to main content Global Regions WHO Regional websites Africa Americas South-East Asia E\n",
            "\n",
            "Number of tokens after preprocessing: 353\n",
            "Indexed 353 tokens for document ID 16.\n",
            "\n",
            "Extracting links from: https://www.who.int/director-general/speeches\n",
            "Found 224 normalized links.\n",
            "\n",
            "Crawling (17/20): https://www.who.int/campaigns\n",
            "Title Extracted: WHO global health days and campaigns\n",
            "Snippet Extracted: WHO global health days and campaigns Skip to main content World Health Organization Global Regions W\n",
            "\n",
            "Number of tokens after preprocessing: 579\n",
            "Indexed 579 tokens for document ID 17.\n",
            "\n",
            "Extracting links from: https://www.who.int/campaigns\n",
            "Found 244 normalized links.\n",
            "\n",
            "Crawling (18/20): https://www.who.int/campaigns/world-ntd-day/2025\n",
            "Title Extracted: World NTD Day 2025\n",
            "Snippet Extracted: World NTD Day 2025 Skip to main content Global Regions WHO Regional websites Africa Americas South-E\n",
            "\n",
            "Number of tokens after preprocessing: 686\n",
            "Indexed 686 tokens for document ID 18.\n",
            "\n",
            "Extracting links from: https://www.who.int/campaigns/world-ntd-day/2025\n",
            "Found 251 normalized links.\n",
            "\n",
            "Crawling (19/20): https://www.who.int/campaigns/world-hearing-day/2025\n",
            "Title Extracted: World Hearing Day 2025\n",
            "Snippet Extracted: World Hearing Day 2025 Skip to main content Global Regions WHO Regional websites Africa Americas Sou\n",
            "\n",
            "Number of tokens after preprocessing: 800\n",
            "Indexed 800 tokens for document ID 19.\n",
            "\n",
            "Extracting links from: https://www.who.int/campaigns/world-hearing-day/2025\n",
            "Found 248 normalized links.\n",
            "\n",
            "Crawling (20/20): https://www.who.int/about/contact-us\n",
            "Title Extracted: Contact the World Health Organization\n",
            "Snippet Extracted: Contact the World Health Organization Skip to main content Global Regions WHO Regional websites Afri\n",
            "\n",
            "Number of tokens after preprocessing: 594\n",
            "Indexed 594 tokens for document ID 20.\n",
            "\n",
            "Extracting links from: https://www.who.int/about/contact-us\n",
            "Found 245 normalized links.\n",
            "\n",
            "Finished crawling. Total pages crawled: 20\n",
            "Total unique words indexed: 1524\n",
            "\n",
            "Exporting search results to search_results.xlsx...\n",
            "Search results have been exported to search_results.xlsx\n",
            "\n",
            "Ranking words based on total frequency across all documents...\n",
            "Word ranking completed.\n",
            "\n",
            "Building inverted index for the top 15 words and exporting to inverted_index.xlsx...\n",
            "Inverted index has been exported to inverted_index.xlsx\n",
            "\n",
            "\n",
            "===== Results for Query: \"Types of COVID-19 Treatment\" =====\n",
            "\n",
            " Rank  Score                                                                                                               Title                                                                                                                                                                      URL                                               Snippet\n",
            "    1 0.0051                                                                               Georgia certified malaria-free by WHO                                                                                           https://www.who.int/news/item/23-01-2025-georgia-certified-malaria-free-by-who Georgia certified malaria-free by WHO Skip to main...\n",
            "    2 0.0049 WHO Director-General's opening remarks at the media briefing on outbreak of Marburg virus disease – 20 January 2025 https://www.who.int/director-general/speeches/detail/who-director-general-s-opening-remarks-at-the-media-briefing-on-outbreak-of-marburg-virus-disease---20-january-2025 WHO Director-General's opening remarks at the medi...\n",
            "    3 0.0033                                                                    Coronavirus Disease (COVID-19) Situation Reports                                                                                        https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports Coronavirus Disease (COVID-19) Situation Reports S...\n",
            "    4 0.0026                                                                                                       Invest in WHO                                                                                                                          https://www.who.int/about/funding/invest-in-who Invest in WHO Skip to main content Global Regions ...\n",
            "    5 0.0022                                                                                                  World NTD Day 2025                                                                                                                         https://www.who.int/campaigns/world-ntd-day/2025 World NTD Day 2025 Skip to main content Global Reg...\n",
            "\n",
            "\n",
            "\n",
            "===== Results for Query: \"guidelines on antibiotic resistance\" =====\n",
            "\n",
            " Rank  Score                                            Title                                                                               URL                                               Snippet\n",
            "    1 0.0108  World Health Organization Emergencies Programme                                          https://www.who.int/emergencies/overview World Health Organization Emergencies Programme Sk...\n",
            "    2 0.0072 Coronavirus Disease (COVID-19) Situation Reports https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports Coronavirus Disease (COVID-19) Situation Reports S...\n",
            "    3 0.0041                                     Publications                                                  https://www.who.int/publications Publications Skip to main content Global Regions W...\n",
            "    4 0.0028                                    Mpox outbreak                          https://www.who.int/emergencies/situations/mpox-outbreak Mpox outbreak Skip to main content Global Regions ...\n",
            "\n",
            "\n",
            "\n",
            "===== Results for Query: \"mental health resources during emergencies\" =====\n",
            "\n",
            " Rank  Score                                                                                                               Title                                                                                                                                                                      URL                                               Snippet\n",
            "    1 0.0989                   WHO launches US$ 1.5 billion Health Emergency Appeal to tackle unprecedented global health crises                                https://www.who.int/news/item/16-01-2025-who-launches-us-1.5-billion-health-emergency-appeal-to-tackle-unprecedented-global-health-crises WHO launches US$ 1.5 billion Health Emergency Appe...\n",
            "    2 0.0949                                                                     World Health Organization Emergencies Programme                                                                                                                                 https://www.who.int/emergencies/overview World Health Organization Emergencies Programme Sk...\n",
            "    3 0.0593 WHO Director-General's opening remarks at the media briefing on outbreak of Marburg virus disease – 20 January 2025 https://www.who.int/director-general/speeches/detail/who-director-general-s-opening-remarks-at-the-media-briefing-on-outbreak-of-marburg-virus-disease---20-january-2025 WHO Director-General's opening remarks at the medi...\n",
            "    4 0.0590                                                                                                           About WHO                                                                                                                                                https://www.who.int/about About WHO Skip to main content Global Regions WHO ...\n",
            "    5 0.0579                    The ceasefire in Gaza brings hope, but immense challenges lie ahead to restore the health system                                https://www.who.int/news/item/19-01-2025-the-ceasefire-in-gaza-brings-hope--but-immense-challenges-lie-ahead-to-restore-the-health-system The ceasefire in Gaza brings hope, but immense cha...\n",
            "    6 0.0575                                                                                WHO global health days and campaigns                                                                                                                                            https://www.who.int/campaigns WHO global health days and campaigns Skip to main ...\n",
            "    7 0.0554                                                                     WHO Western Pacific | World Health Organization                                                                                                                                       https://www.who.int/westernpacific WHO Western Pacific | World Health Organization Sk...\n",
            "    8 0.0533                                                   WHO comments on United States’ announcement of intent to withdraw                                                               https://www.who.int/news/item/21-01-2025-who-comments-on-united-states--announcement-of-intent-to-withdraw WHO comments on United States’ announcement of int...\n",
            "    9 0.0518                                                                                                       Invest in WHO                                                                                                                          https://www.who.int/about/funding/invest-in-who Invest in WHO Skip to main content Global Regions ...\n",
            "   10 0.0503                                                                               Countries | World Health Organization                                                                                                                                            https://www.who.int/countries Countries | World Health Organization Skip to main...\n",
            "   11 0.0499                                                                                                            Speeches                                                                                                                            https://www.who.int/director-general/speeches Speeches Skip to main content Global Regions WHO R...\n",
            "   12 0.0489                                                                                     World Health Organization (WHO)                                                                                                                                                      https://www.who.int World Health Organization (WHO) Skip to main conte...\n",
            "   13 0.0454                                                                    Coronavirus Disease (COVID-19) Situation Reports                                                                                        https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports Coronavirus Disease (COVID-19) Situation Reports S...\n",
            "   14 0.0422                                                                                                       Mpox outbreak                                                                                                                 https://www.who.int/emergencies/situations/mpox-outbreak Mpox outbreak Skip to main content Global Regions ...\n",
            "   15 0.0404                                                                                                        Publications                                                                                                                                         https://www.who.int/publications Publications Skip to main content Global Regions W...\n",
            "   16 0.0394                                                                               Georgia certified malaria-free by WHO                                                                                           https://www.who.int/news/item/23-01-2025-georgia-certified-malaria-free-by-who Georgia certified malaria-free by WHO Skip to main...\n",
            "   17 0.0385                                                                                                  World NTD Day 2025                                                                                                                         https://www.who.int/campaigns/world-ntd-day/2025 World NTD Day 2025 Skip to main content Global Reg...\n",
            "   18 0.0346                                                                               Contact the World Health Organization                                                                                                                                     https://www.who.int/about/contact-us Contact the World Health Organization Skip to main...\n",
            "   19 0.0295                                                                            Episode #133 - Measles: a growing threat                                                                               https://www.who.int/podcasts/episode/science-in-5/episode--133---measles--a-growing-threat Episode #133 - Measles: a growing threat Skip to m...\n",
            "   20 0.0281                                                                                              World Hearing Day 2025                                                                                                                     https://www.who.int/campaigns/world-hearing-day/2025 World Hearing Day 2025 Skip to main content Global...\n",
            "\n",
            "\n",
            "All term-doc appearances saved to term_doc_appearance_all_terms.xlsx\n",
            "Done! Elapsed time: 8.01 seconds.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "print(\"Downloading NLTK resources...\")\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "print(\"NLTK resources downloaded.\\n\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Text preprocessing functions\n",
        "def remove_stop_words(text_tokens):\n",
        "    \"\"\"Remove standard English stopwords and keep only alphabetic tokens.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in text_tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "    return words\n",
        "\n",
        "def trim_words(words):\n",
        "    \"\"\"Trim punctuation from ends of words.\"\"\"\n",
        "    cleaned_words = [word.rstrip(\",.\\\\/?!'\\\"\") for word in words]\n",
        "    return cleaned_words\n",
        "\n",
        "def apply_stemming(words):\n",
        "    \"\"\"Apply Porter stemming to tokens.\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word.lower()) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def apply_lemmatization(words):\n",
        "    \"\"\"Apply lemmatization to tokens.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "    return lemmatized_words\n",
        "\n",
        "def get_links(url):\n",
        "    \"\"\"\n",
        "    Extract all absolute (normalized) links from a webpage.\n",
        "    Only returns http/https links.\n",
        "    \"\"\"\n",
        "    print(f\"Extracting links from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
        "\n",
        "        normalized_links = []\n",
        "        for link in links:\n",
        "            if link.startswith('/'):\n",
        "                # Convert relative URL to absolute\n",
        "                normalized_link = urljoin(url, link)\n",
        "                normalized_links.append(normalized_link)\n",
        "            elif link.startswith('http'):\n",
        "                normalized_links.append(link)\n",
        "        print(f\"Found {len(normalized_links)} normalized links.\\n\")\n",
        "        return normalized_links\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting links from {url}: {e}\\n\")\n",
        "        return []\n",
        "\n",
        "def create_index(start_url, max_pages=50):\n",
        "    \"\"\"\n",
        "    Crawl up to `max_pages` starting from `start_url`.\n",
        "    Build:\n",
        "      - index: word -> {doc_id: term_frequency_in_doc}\n",
        "      - results: list of metadata for each page (URL, Title, Snippet, FullText)\n",
        "      - doc_id_to_url: map doc_id -> URL\n",
        "      - doc_length: map doc_id -> total number of preprocessed words (for TF normalization)\n",
        "    \"\"\"\n",
        "\n",
        "    # Terms to exclude from any URL\n",
        "    excluded_terms = [\n",
        "        \"/mega-menu\",\n",
        "        \"/services/certifications\",\n",
        "        \"/login\",\n",
        "        \"/subscribe\",\n",
        "        \"/facebook\",\n",
        "        \"/twitter\",\n",
        "        \"/instagram\",\n",
        "        \"/youtube\",\n",
        "        \"/search?\",\n",
        "        \"/draft\",\n",
        "        \"/template\",\n",
        "        \"/newsletters\",\n",
        "        \"/maldives\",\n",
        "        \"/footer\",\n",
        "        \"/southeastasia\",\n",
        "        \"/south-east-asia\",\n",
        "        \"/europe\",\n",
        "        \"/health-topics\",\n",
        "        \"/about-us\",\n",
        "        \"/news-room\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    index = defaultdict(dict)\n",
        "    visited = set()\n",
        "    queue = [start_url]\n",
        "    results = []\n",
        "    pages_crawled = 0\n",
        "    link_ids = {}\n",
        "    doc_length = {}\n",
        "\n",
        "    while queue and pages_crawled < max_pages:\n",
        "        current_url = queue.pop(0)\n",
        "\n",
        "        # Normalize the URL\n",
        "        parsed_url = urlparse(current_url)\n",
        "        normalized_url = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path.rstrip('/')\n",
        "\n",
        "        if normalized_url in visited:\n",
        "            continue\n",
        "\n",
        "        visited.add(normalized_url)\n",
        "        pages_crawled += 1\n",
        "        print(f\"Crawling ({pages_crawled}/{max_pages}): {normalized_url}\")\n",
        "\n",
        "        try:\n",
        "            session = requests.Session()\n",
        "            session.max_redirects = 2\n",
        "            response = session.get(normalized_url, allow_redirects=True, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract title\n",
        "            title = soup.title.string.strip() if soup.title else 'No Title'\n",
        "\n",
        "            # Extract text for indexing\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            snippet = text[:200] + '...' if len(text) > 200 else text\n",
        "\n",
        "            # Assign doc_id for this page\n",
        "            doc_id = pages_crawled\n",
        "            link_ids[normalized_url] = doc_id\n",
        "\n",
        "            # Store metadata\n",
        "            results.append({\n",
        "                'URL': normalized_url,\n",
        "                'Title': title,\n",
        "                'Snippet': snippet,\n",
        "                'FullText': text\n",
        "            })\n",
        "\n",
        "            print(f\"Title Extracted: {title}\")\n",
        "            print(f\"Snippet Extracted: {snippet[:100]}\\n\")\n",
        "\n",
        "            # Preprocess tokens (stopword removal, stemming, etc.)\n",
        "            raw_tokens = text.split()\n",
        "            cleaned_tokens = remove_stop_words(raw_tokens)\n",
        "            cleaned_tokens = trim_words(cleaned_tokens)\n",
        "            cleaned_tokens = apply_stemming(cleaned_tokens)\n",
        "\n",
        "            doc_length[doc_id] = len(cleaned_tokens)\n",
        "            print(f\"Number of tokens after preprocessing: {doc_length[doc_id]}\")\n",
        "\n",
        "            # Update inverted index\n",
        "            for token in cleaned_tokens:\n",
        "                index[token].setdefault(doc_id, 0)\n",
        "                index[token][doc_id] += 1\n",
        "\n",
        "            print(f\"Indexed {len(cleaned_tokens)} tokens for document ID {doc_id}.\\n\")\n",
        "\n",
        "            # Extract links and enqueue children\n",
        "            extracted_links = get_links(normalized_url)\n",
        "            for link in extracted_links:\n",
        "                # Skip excluded terms\n",
        "                if any(excluded in link for excluded in excluded_terms):\n",
        "                    continue\n",
        "\n",
        "                # You can also restrict to the base domain if desired:\n",
        "                if link.startswith(start_url) and link not in visited:\n",
        "                    queue.append(link)\n",
        "\n",
        "        except requests.Timeout:\n",
        "            print(f\"Timeout: {normalized_url}\\n\")\n",
        "            continue\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"RequestException: {e} for URL: {normalized_url}\\n\")\n",
        "            continue\n",
        "\n",
        "    doc_id_to_url = {v: k for k, v in link_ids.items()}\n",
        "\n",
        "    print(f\"Finished crawling. Total pages crawled: {pages_crawled}\")\n",
        "    print(f\"Total unique words indexed: {len(index)}\\n\")\n",
        "\n",
        "    return index, results, doc_id_to_url, doc_length\n",
        "\n",
        "\n",
        "def compute_idf(index, total_docs):\n",
        "    \"\"\"\n",
        "    Compute Inverse Document Frequency for each term in the index.\n",
        "    IDF(term) = log( total_docs / (df + 1) ) + 1\n",
        "      - 'df' is the number of docs containing the term\n",
        "    Returns a dictionary: { term: idf_value }\n",
        "    \"\"\"\n",
        "    idf_dict = {}\n",
        "    for term, posting_dict in index.items():\n",
        "        df = len(posting_dict)  # number of docs that contain this term\n",
        "        # Add 1 to avoid division by zero if df=0\n",
        "        idf_value = math.log((total_docs / (df + 1)), 10) + 1\n",
        "        idf_dict[term] = idf_value\n",
        "    return idf_dict\n",
        "\n",
        "\n",
        "def search(query, index, idf_dict, doc_length, doc_id_to_url, search_results, top_k=10):\n",
        "    \"\"\"\n",
        "    Given a query string, rank documents by TF-IDF score.\n",
        "    Return the top_k results as a list of dicts: { 'doc_id', 'score', 'URL', 'Title', 'Snippet' }.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Preprocess the query ---\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    raw_query_tokens = query.split()\n",
        "    query_tokens = [\n",
        "        stemmer.stem(w.lower()) for w in raw_query_tokens\n",
        "        if w.lower() not in stop_words and w.isalpha()\n",
        "    ]\n",
        "\n",
        "    if not query_tokens:\n",
        "        print(\"No valid tokens in the query after preprocessing.\")\n",
        "        return []\n",
        "\n",
        "    # --- Accumulate scores ---\n",
        "    scores = defaultdict(float)  # doc_id -> score\n",
        "\n",
        "    # For each token in the query, update the TF-IDF score for each doc that has it\n",
        "    for token in query_tokens:\n",
        "        if token not in index:\n",
        "            continue  # Token not in any doc\n",
        "        posting_list = index[token]\n",
        "        for doc_id, freq in posting_list.items():\n",
        "            # TF = frequency_in_doc / doc_length\n",
        "            tf = freq / doc_length[doc_id]\n",
        "            # IDF for the token\n",
        "            idf = idf_dict[token]\n",
        "            # Accumulate\n",
        "            scores[doc_id] += (tf * idf)\n",
        "\n",
        "    # --- Sort docs by score descending ---\n",
        "    # We only want docs that have a nonzero score\n",
        "    ranked_doc_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # --- Build final results ---\n",
        "    # Convert doc_id -> metadata (title, snippet, etc.)\n",
        "    result_docs = []\n",
        "    for doc_id, score in ranked_doc_ids[:top_k]:\n",
        "        # Find the corresponding entry in search_results\n",
        "        # or build from doc_id_to_url\n",
        "        url = doc_id_to_url[doc_id]\n",
        "        # Attempt to get the snippet/title from search_results\n",
        "        # We can do a quick lookup:\n",
        "        metadata = next((d for d in search_results if d['URL'] == url), None)\n",
        "        if metadata:\n",
        "            result_docs.append({\n",
        "                'doc_id': doc_id,\n",
        "                'score': score,\n",
        "                'URL': url,\n",
        "                'Title': metadata['Title'],\n",
        "                'Snippet': metadata['Snippet'],\n",
        "            })\n",
        "        else:\n",
        "            result_docs.append({\n",
        "                'doc_id': doc_id,\n",
        "                'score': score,\n",
        "                'URL': url,\n",
        "                'Title': \"No Title\",\n",
        "                'Snippet': \"\",\n",
        "            })\n",
        "\n",
        "    return result_docs\n",
        "\n",
        "def create_ranked_words(index):\n",
        "    \"\"\"\n",
        "    Return a dict sorted by total frequency across all docs\n",
        "    word -> { 'rank': rank, 'counter': total_frequency_across_docs }\n",
        "    \"\"\"\n",
        "    print(\"Ranking words based on total frequency across all documents...\")\n",
        "    sorted_dict = {}\n",
        "    for word, doc_dict in index.items():\n",
        "        total_count = sum(doc_dict.values())\n",
        "        sorted_dict[word] = total_count\n",
        "\n",
        "    sorted_dict = dict(sorted(sorted_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    ranked_dict = {}\n",
        "    rank = 1\n",
        "    for word, counter in sorted_dict.items():\n",
        "        ranked_dict[word] = {'rank': rank, 'counter': counter}\n",
        "        rank += 1\n",
        "\n",
        "    print(\"Word ranking completed.\\n\")\n",
        "    return ranked_dict\n",
        "\n",
        "def export_search_results(search_results, filename='search_results.xlsx'):\n",
        "    print(f\"Exporting search results to {filename}...\")\n",
        "    df = pd.DataFrame(search_results)\n",
        "    try:\n",
        "        df.to_excel(filename, index=False)\n",
        "        print(f\"Search results have been exported to {filename}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error exporting search results: {e}\")\n",
        "\n",
        "def export_inverted_index(index, chosen_words, filename='inverted_index.xlsx'):\n",
        "    print(f\"Building inverted index for the top {len(chosen_words)} words and exporting to {filename}...\")\n",
        "    inverted_index_data = []\n",
        "    for word in chosen_words:\n",
        "        doc_dict = index[word]\n",
        "        # For large corpora, you might limit docs displayed\n",
        "        for doc_id, count in doc_dict.items():\n",
        "            inverted_index_data.append({'Word': word, 'Document_ID': doc_id, 'Term_Frequency': count})\n",
        "    df_inverted = pd.DataFrame(inverted_index_data)\n",
        "    try:\n",
        "        df_inverted.to_excel(filename, index=False)\n",
        "        print(f\"Inverted index has been exported to {filename}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error exporting inverted index: {e}\")\n",
        "\n",
        "# Optional helpers to export term-doc matrices, etc.\n",
        "def create_term_doc_excel_with_query(index, doc_id_to_url, query, file_path=\"term_doc_appearance_query.xlsx\"):\n",
        "    \"\"\"\n",
        "    Creates an Excel file listing term frequencies in each doc for terms in the given query.\n",
        "    Rows = query terms\n",
        "    Columns = document URLs\n",
        "    \"\"\"\n",
        "\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # --- 1. Preprocess the query ---\n",
        "    raw_query_tokens = query.split()\n",
        "    processed_query_terms = [\n",
        "        stemmer.stem(word.lower())\n",
        "        for word in raw_query_tokens\n",
        "        if word.lower() not in stop_words and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Keep only the terms that actually appear in our index\n",
        "    relevant_terms = [term for term in processed_query_terms if term in index]\n",
        "    if not relevant_terms:\n",
        "        print(\"No relevant terms from the query found in the index.\")\n",
        "        return\n",
        "\n",
        "    # --- 2. Identify all doc IDs (so we can create columns) ---\n",
        "    all_doc_ids = sorted({doc_id for posting in index.values() for doc_id in posting})\n",
        "\n",
        "    # --- 3. Prepare data dictionary for DataFrame ---\n",
        "    # We'll have one row per query term, plus one column \"Term\"\n",
        "    data = {\"Term\": relevant_terms}\n",
        "\n",
        "    # Create an empty list for each doc's column\n",
        "    for doc_id in all_doc_ids:\n",
        "        doc_url = doc_id_to_url.get(doc_id, f\"Doc {doc_id}\")\n",
        "        data[doc_url] = []  # Each column is initially empty\n",
        "\n",
        "    # --- 4. Fill in frequencies for each (term, doc) pair ---\n",
        "    for term in relevant_terms:\n",
        "        for doc_id in all_doc_ids:\n",
        "            freq = index[term].get(doc_id, 0)\n",
        "            doc_url = doc_id_to_url.get(doc_id, f\"Doc {doc_id}\")\n",
        "            data[doc_url].append(freq)\n",
        "\n",
        "    # --- 5. Create and export DataFrame ---\n",
        "    df = pd.DataFrame(data)\n",
        "    try:\n",
        "        df.to_excel(file_path, index=False)\n",
        "        print(f\"Term-doc appearance for the query saved to '{file_path}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving term-doc appearance: {e}\")\n",
        "\n",
        "def create_term_doc_excel_for_all_terms(index, doc_id_to_url, file_path=\"term_doc_appearance_all_terms.xlsx\"):\n",
        "    \"\"\"Creates an Excel file listing term frequencies for all terms in each doc.\"\"\"\n",
        "    all_terms = sorted(index.keys())\n",
        "    all_doc_ids = sorted({doc_id for term_docs in index.values() for doc_id in term_docs})\n",
        "    data = {\"term/doc\": []}\n",
        "    urls = [doc_id_to_url.get(doc_id, f\"Doc {doc_id}\") for doc_id in all_doc_ids]\n",
        "\n",
        "    for url in urls:\n",
        "        data[url] = []\n",
        "\n",
        "    for term in all_terms:\n",
        "        data[\"term/doc\"].append(term)\n",
        "        for doc_id_idx, doc_id in enumerate(all_doc_ids):\n",
        "            col_url = urls[doc_id_idx]\n",
        "            tf = index[term].get(doc_id, 0)\n",
        "            data[col_url].append(tf)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    try:\n",
        "        df.to_excel(file_path, index=False)\n",
        "        print(f\"All term-doc appearances saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving all terms doc matrix: {e}\")\n",
        "\n",
        "\n",
        "def pretty_print_results(query, results):\n",
        "    \"\"\"\n",
        "    Display search results in a neat table using pandas.\n",
        "    Each row: Rank, Score, Title, URL, Snippet\n",
        "    \"\"\"\n",
        "    # Prepare a list of dict rows to feed into a DataFrame\n",
        "    rows = []\n",
        "    for rank, item in enumerate(results, start=1):\n",
        "        rows.append({\n",
        "            'Rank': rank,\n",
        "            'Score': round(item['score'], 4),\n",
        "            'Title': item['Title'],\n",
        "            'URL': item['URL'],\n",
        "            'Snippet': item['Snippet'][:50] + \"...\"  # Just show first ~50 chars\n",
        "        })\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(rows, columns=['Rank', 'Score', 'Title', 'URL', 'Snippet'])\n",
        "    print(f\"\\n===== Results for Query: \\\"{query}\\\" =====\\n\")\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# =========================\n",
        "# Main Execution Example\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 1. Crawl & build index\n",
        "    website_url = 'https://www.who.int'\n",
        "    max_pages_to_crawl = 20\n",
        "    index, search_results, doc_id_to_url, doc_length = create_index(website_url, max_pages=max_pages_to_crawl)\n",
        "\n",
        "    # 2. Compute IDF for the corpus\n",
        "    total_docs = len(doc_id_to_url)\n",
        "    idf_dict = compute_idf(index, total_docs)\n",
        "\n",
        "    # 3. Export search results\n",
        "    export_search_results(search_results, filename='search_results.xlsx')\n",
        "\n",
        "    # 4. Create a word frequency ranking\n",
        "    ranked_words = create_ranked_words(index)\n",
        "    # Let's pick some subset if you want\n",
        "    chosen_words = list(ranked_words.keys())[:15]\n",
        "    export_inverted_index(index, chosen_words, filename='inverted_index.xlsx')\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        'Types of COVID-19 Treatment',\n",
        "        \"guidelines on antibiotic resistance\",\n",
        "        \"mental health resources during emergencies\"\n",
        "    ]\n",
        "\n",
        "    for q in queries:\n",
        "        # top_k can be adjusted to show however many results you want\n",
        "        top_k = 3000\n",
        "        results_for_query = search(\n",
        "            q,\n",
        "            index,\n",
        "            idf_dict,\n",
        "            doc_length,\n",
        "            doc_id_to_url,\n",
        "            search_results,\n",
        "            top_k=top_k\n",
        "        )\n",
        "        pretty_print_results(q, results_for_query)\n",
        "\n",
        "    # [Optional] Export full term-doc matrix\n",
        "    create_term_doc_excel_for_all_terms(index, doc_id_to_url, file_path=\"term_doc_appearance_all_terms.xlsx\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Done! Elapsed time: {end_time - start_time:.2f} seconds.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afXQLt_4bJe-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBn1pWDR2NvX"
      },
      "source": [
        "Algorithm for checking which retrieved pages are linked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ2YF4Ol2_g_"
      },
      "source": [
        "Query 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRAVPIG_oBAI",
        "outputId": "f51a50df-1a22-41f1-e50d-7473c45d483f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.who.int/news-room/fact-sheets/detail/hiv-drug-resistance ---> https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance\n",
            "https://www.who.int/publications/who-guidelines ---> https://www.who.int/publications/i/item/guidelines-for-malaria\n",
            "https://www.who.int/publications/who-guidelines ---> https://www.who.int/publications/i/item/guidelines-for-malaria\n",
            "https://www.who.int/news-room/fact-sheets/detail/multi-drug-resistant-gonorrhoea ---> https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance\n",
            "https://www.who.int/campaigns/world-amr-awareness-week ---> https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "ret_urls = [\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/hiv-drug-resistance\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-11---antibiotics-covid-19\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/microbes-are-becoming-resistant-to-antibiotics\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-61---covid-19-antibiotics\",\n",
        "    \"https://www.who.int/westernpacific/newsroom/events/world-antibiotic-awareness-week\",\n",
        "    \"https://www.who.int/publications/who-guidelines\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/multi-drug-resistant-gonorrhoea\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-99-three-things-to-keep-in-mind-when-taking-antibiotics\",\n",
        "    \"https://www.who.int/publications/i/item/guidelines-for-malaria\",\n",
        "    \"https://www.who.int/campaigns/world-amr-awareness-week\",\n",
        "    \"https://www.who.int/westernpacific/activities/tackling-antimicrobial-resistance\",\n",
        "    \"https://www.who.int/guam/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/americansamoa/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/tonga/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/vanuatu/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/wallisandfutuna/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/antibiotic-resistance\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance\",\n",
        "    \"https://www.who.int/kiribati/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/tuvalu/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\"\n",
        "]\n",
        "\n",
        "for url in ret_urls:\n",
        "\n",
        "  # URL of the webpage\n",
        "  current_url = url\n",
        "\n",
        "  # Send a request to the webpage\n",
        "  response = requests.get(current_url)\n",
        "\n",
        "  # Parse the HTML\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # Find all <a> tags\n",
        "  links = soup.find_all(\"a\")\n",
        "\n",
        "  # Extract and print the href attributes\n",
        "  for link in links:\n",
        "      href = link.get(\"href\")\n",
        "      if href and \"https\" in href and href in ret_urls:\n",
        "          print(current_url + \" ---> \" + href)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGDlH8-vr_Q6",
        "outputId": "77f9dc31-a8ae-4229-cd65-f50b90d1f490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.who.int/news-room/fact-sheets/detail/hiv-drug-resistance : [https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance]\n",
            "https://www.who.int/publications/who-guidelines : [https://www.who.int/publications/i/item/guidelines-for-malaria]\n",
            "https://www.who.int/publications/who-guidelines : [https://www.who.int/publications/i/item/guidelines-for-malaria]\n",
            "https://www.who.int/news-room/fact-sheets/detail/multi-drug-resistant-gonorrhoea : [https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance]\n",
            "https://www.who.int/campaigns/world-amr-awareness-week : [https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "ret_urls = [\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/hiv-drug-resistance\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-11---antibiotics-covid-19\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/microbes-are-becoming-resistant-to-antibiotics\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-61---covid-19-antibiotics\",\n",
        "    \"https://www.who.int/westernpacific/newsroom/events/world-antibiotic-awareness-week\",\n",
        "    \"https://www.who.int/publications/who-guidelines\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/multi-drug-resistant-gonorrhoea\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-99-three-things-to-keep-in-mind-when-taking-antibiotics\",\n",
        "    \"https://www.who.int/publications/i/item/guidelines-for-malaria\",\n",
        "    \"https://www.who.int/campaigns/world-amr-awareness-week\",\n",
        "    \"https://www.who.int/westernpacific/activities/tackling-antimicrobial-resistance\",\n",
        "    \"https://www.who.int/guam/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/americansamoa/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/tonga/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/vanuatu/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/wallisandfutuna/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/antibiotic-resistance\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance\",\n",
        "    \"https://www.who.int/kiribati/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\",\n",
        "    \"https://www.who.int/tuvalu/news/feature-stories/item/combating-antimicrobial-resistance-in-the-pacific\"\n",
        "]\n",
        "\n",
        "for url in ret_urls:\n",
        "\n",
        "  # URL of the webpage\n",
        "  current_url = url\n",
        "\n",
        "  # Send a request to the webpage\n",
        "  response = requests.get(current_url)\n",
        "\n",
        "  # Parse the HTML\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # Find all <a> tags\n",
        "  links = soup.find_all(\"a\")\n",
        "\n",
        "  # Extract and print the href attributes\n",
        "  for link in links:\n",
        "      href = link.get(\"href\")\n",
        "      if href and \"https\" in href and href in ret_urls:\n",
        "          print(current_url + \" : \" + \"[\" + href + \"]\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTu9TdBr3GMH"
      },
      "source": [
        "Query 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcs55Fw8s7gV",
        "outputId": "8e06a58f-830f-42fa-83da-3bccafa3b6be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.who.int/news-room/fact-sheets/detail/lung-cancer ---> https://www.who.int/news-room/fact-sheets/detail/cancer\n",
            "https://www.who.int/news-room/fact-sheets/detail/cancer ---> https://www.who.int/news-room/fact-sheets/detail/colorectal-cancer\n",
            "https://www.who.int/news-room/fact-sheets/detail/cancer ---> https://www.who.int/news-room/fact-sheets/detail/lung-cancer\n",
            "https://www.who.int/news-room/fact-sheets/detail/colorectal-cancer ---> https://www.who.int/news-room/fact-sheets/detail/cancer\n",
            "https://www.who.int/news-room/fact-sheets/detail/colorectal-cancer ---> https://www.who.int/news-room/fact-sheets/detail/lung-cancer\n",
            "https://www.who.int/news-room/fact-sheets/detail/mycetoma ---> https://www.who.int/news-room/fact-sheets/detail/leprosy\n",
            "https://www.who.int/news-room/fact-sheets/detail/leprosy ---> https://www.who.int/news-room/fact-sheets/detail/mycetoma\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "ret_urls = [\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/diabetes\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/lung-cancer\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/schistosomiasis\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/hepatitis-c\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/depression\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/human-papilloma-virus-and-cancer\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/cancer\",\n",
        "    \"https://www.who.int/westernpacific/activities/improving-access-to-prevention-testing-and-treatment-for-hepatitis\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/colorectal-cancer\",\n",
        "    \"https://www.who.int/publications/i/item/9789240096745\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-46---diabetes-covid-19\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/endometriosis\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-68-covid-19-update-on-long-covid\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/mycetoma\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/tuberculosis\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/leprosy\",\n",
        "    \"https://www.who.int/news/item/08-01-2025-who-prequalifies-diagnostic-test-to-support-safer-administration-of-p.-vivax-malaria-treatments\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/ebola-virus-disease\",\n",
        "    \"https://www.who.int/news/item/23-04-2024-promising-patient-friendly-oral-drug-against-visceral-leishmaniasis-enters-phase-ii-clinical-trial-in-ethiopia\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-37---treatment-and-care-at-home\"\n",
        "]\n",
        "\n",
        "for url in ret_urls:\n",
        "\n",
        "  # URL of the webpage\n",
        "  current_url = url\n",
        "\n",
        "  # Send a request to the webpage\n",
        "  response = requests.get(current_url)\n",
        "\n",
        "  # Parse the HTML\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # Find all <a> tags\n",
        "  links = soup.find_all(\"a\")\n",
        "\n",
        "  # Extract and print the href attributes\n",
        "  for link in links:\n",
        "      href = link.get(\"href\")\n",
        "      if href and \"https\" in href and href in ret_urls:\n",
        "          print(current_url + \" ---> \" + href)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vrcavdz3JrP"
      },
      "source": [
        "**Query** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqF6TppFt1Be",
        "outputId": "1d45616f-07b1-4324-baaa-6cb06901ded9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.who.int/westernpacific/activities/communicating-risk-in-public-health-emergencies ---> https://www.who.int/westernpacific/activities/communicating-risk-in-public-health-emergencies\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "ret_urls = [\n",
        "    \"https://www.who.int/teams/health-workforce/PHEworkforce\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/hepatitis-outbreak-in-children\",\n",
        "    \"https://www.who.int/publications/b\",\n",
        "    \"https://www.who.int/malaysia/emergencies/covid-19-in-malaysia/information\",\n",
        "    \"https://www.who.int/publications/i/item/9789240090743\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-71---covid-19-vaccines-and-children\",\n",
        "    \"https://www.who.int/emergencies/operations\",\n",
        "    \"https://www.who.int/activities/measuring-the-effectiveness-and-impact-of-public-health-and-social-measures\",\n",
        "    \"https://www.who.int/westernpacific/activities/communicating-risk-in-public-health-emergencies\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/public-health-emergency-of-international-concern\",\n",
        "    \"https://www.who.int/mongolia/multi-media/item/live-life-safely--trains-and-public-transport\",\n",
        "    \"https://www.who.int/westernpacific/our-work/resources/publications\",\n",
        "    \"https://www.who.int/westernpacific/publications\",\n",
        "    \"https://www.who.int/westernpacific/activities/detecting-and-assessing-emergency-health-threats\",\n",
        "    \"https://www.who.int/emergencies/diseases/novel-coronavirus-2019/media-resources/science-in-5/episode-43---pregnancy-covid-19\",\n",
        "    \"https://www.who.int/publications/i/9789240081277\",\n",
        "    \"https://www.who.int/about/policies/publishing/open-access\",\n",
        "    \"https://www.who.int/westernpacific/publications/i\",\n",
        "    \"https://www.who.int/americansamoa/publications\",\n",
        "    \"https://www.who.int/brunei/publications-hub\"\n",
        "]\n",
        "\n",
        "for url in ret_urls:\n",
        "\n",
        "  # URL of the webpage\n",
        "  current_url = url\n",
        "\n",
        "  # Send a request to the webpage\n",
        "  response = requests.get(current_url)\n",
        "\n",
        "  # Parse the HTML\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # Find all <a> tags\n",
        "  links = soup.find_all(\"a\")\n",
        "\n",
        "  # Extract and print the href attributes\n",
        "  for link in links:\n",
        "      href = link.get(\"href\")\n",
        "      if href and \"https\" in href and href in ret_urls:\n",
        "          print(current_url + \" ---> \" + href)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXKeDABV3MQB"
      },
      "source": [
        "Query 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLgV2NyZuYiS",
        "outputId": "29766cee-0d8c-48d9-eadf-5a06cb95e9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.who.int/campaigns/world-mental-health-day : [https://www.who.int/news-room/fact-sheets/detail/mental-health-strengthening-our-response]\n",
            "https://www.who.int/campaigns/world-mental-health-day : [https://www.who.int/news-room/fact-sheets/detail/mental-health-in-emergencies]\n",
            "https://www.who.int/news-room/fact-sheets/detail/mental-health-strengthening-our-response : [https://www.who.int/news-room/fact-sheets/detail/mental-health-and-forced-displacement]\n",
            "https://www.who.int/news-room/fact-sheets/detail/mental-health-strengthening-our-response : [https://www.who.int/news-room/fact-sheets/detail/mental-health-in-emergencies]\n",
            "https://www.who.int/news-room/fact-sheets/detail/mental-health-at-work : [https://www.who.int/news-room/fact-sheets/detail/mental-health-strengthening-our-response]\n",
            "https://www.who.int/news-room/fact-sheets/detail/mental-health-at-work : [https://www.who.int/news-room/fact-sheets/detail/occupational-health--health-workers]\n",
            "https://www.who.int/news-room/fact-sheets/detail/mental-health-in-emergencies : [https://www.who.int/news-room/fact-sheets/detail/mental-health-and-forced-displacement]\n",
            "https://www.who.int/westernpacific/activities/promoting-mental-health : [https://www.who.int/campaigns/world-mental-health-day]\n",
            "https://www.who.int/westernpacific/activities/promoting-mental-health : [https://www.who.int/westernpacific/activities/strengthening-government-action-on-mental-health]\n",
            "https://www.who.int/westernpacific/activities/promoting-mental-health : [https://www.who.int/westernpacific/activities/developing-community-based-mental-health-services]\n",
            "https://www.who.int/westernpacific/activities/promoting-mental-health : [https://www.who.int/westernpacific/activities/protecting-mental-health-during-emergencies]\n",
            "https://www.who.int/westernpacific/activities/protecting-mental-health-during-emergencies : [https://www.who.int/westernpacific/activities/strengthening-government-action-on-mental-health]\n",
            "https://www.who.int/westernpacific/activities/protecting-mental-health-during-emergencies : [https://www.who.int/westernpacific/activities/developing-community-based-mental-health-services]\n",
            "https://www.who.int/westernpacific/activities/protecting-mental-health-during-emergencies : [https://www.who.int/westernpacific/activities/promoting-mental-health]\n",
            "https://www.who.int/westernpacific/activities/developing-community-based-mental-health-services : [https://www.who.int/westernpacific/activities/protecting-mental-health-during-emergencies]\n",
            "https://www.who.int/westernpacific/activities/developing-community-based-mental-health-services : [https://www.who.int/westernpacific/activities/strengthening-government-action-on-mental-health]\n",
            "https://www.who.int/news-room/fact-sheets/detail/mental-health-and-forced-displacement : [https://www.who.int/news-room/fact-sheets/detail/mental-health-in-emergencies]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "ret_urls = [\n",
        "    \"https://www.who.int/campaigns/world-mental-health-day\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/mental-health-strengthening-our-response\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/mental-health-at-work\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/mental-health-in-emergencies\",\n",
        "    \"https://www.who.int/westernpacific/activities/promoting-mental-health\",\n",
        "    \"https://www.who.int/china/news/feature-stories/detail/covid-19-sounding-the-alarm-again-that-mental-health-intervention-is-an-indispensable-health-service\",\n",
        "    \"https://www.who.int/pitcairnislands/emergencies\",\n",
        "    \"https://www.who.int/tokelau/emergencies\",\n",
        "    \"https://www.who.int/westernpacific/activities/protecting-mental-health-during-emergencies\",\n",
        "    \"https://www.who.int/emergencies/operations\",\n",
        "    \"https://www.who.int/westernpacific/activities/strengthening-government-action-on-mental-health\",\n",
        "    \"https://www.who.int/westernpacific/activities/developing-community-based-mental-health-services\",\n",
        "    \"https://www.who.int/fiji/publications-detail/RS-2003-GE-44-TON\",\n",
        "    \"https://www.who.int/teams/health-workforce/PHEworkforce\",\n",
        "    \"https://www.who.int/westernpacific/about/how-we-work/programmes/who-health-emergencies-programme\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/occupational-health--health-workers\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/mental-health-and-forced-displacement\",\n",
        "    \"https://www.who.int/news-room/fact-sheets/detail/health-literacy\",\n",
        "    \"https://www.who.int/malaysia/news/detail/12-09-2024-singapore-contributes-to-regional-health-emergency-readiness-through-achieving-emergency-medical-team-classification\",\n",
        "    \"https://www.who.int/singapore/news/detail/12-09-2024-singapore-contributes-to-regional-health-emergency-readiness-through-achieving-emergency-medical-team-classification\"\n",
        "]\n",
        "for url in ret_urls:\n",
        "  # URL of the webpage\n",
        "  current_url = url\n",
        "  # Send a request to the webpage\n",
        "  response = requests.get(current_url)\n",
        "  # Parse the HTML\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "  # Find all <a> tags\n",
        "  links = soup.find_all(\"a\")\n",
        "  # Extract and print the href attributes\n",
        "  for link in links:\n",
        "      href = link.get(\"href\")\n",
        "      if href and \"https\" in href and href in ret_urls:\n",
        "        print(current_url + \" : \" + \"[\" + href + \"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peurIlXeTFzh",
        "outputId": "194aa9f0-98c2-45b5-f66c-b33be0e65ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Web structure:\n",
            "Page A links to: B, H\n",
            "Page C links to: D\n",
            "Page E links to: F\n",
            "Page G links to: H, I\n",
            "Page J links to: G\n",
            "\n",
            "Initial PageRank Values:\n",
            "\n",
            "PageRank values after iteration 0:\n",
            "-----------------------------------\n",
            "Page  |  PageRank Value\n",
            "-----------------------------------\n",
            "  A   |     0.100\n",
            "  B   |     0.100\n",
            "  C   |     0.100\n",
            "  D   |     0.100\n",
            "  E   |     0.100\n",
            "  F   |     0.100\n",
            "  G   |     0.100\n",
            "  H   |     0.100\n",
            "  I   |     0.100\n",
            "  J   |     0.100\n",
            "-----------------------------------\n",
            "\n",
            "PageRank values after iteration 1:\n",
            "-----------------------------------\n",
            "Page  |  PageRank Value\n",
            "-----------------------------------\n",
            "  A   |     0.026\n",
            "  B   |     0.100\n",
            "  C   |     0.026\n",
            "  D   |     0.174\n",
            "  E   |     0.026\n",
            "  F   |     0.174\n",
            "  G   |     0.174\n",
            "  H   |     0.174\n",
            "  I   |     0.100\n",
            "  J   |     0.026\n",
            "-----------------------------------\n",
            "\n",
            "PageRank values after iteration 2:\n",
            "-----------------------------------\n",
            "Page  |  PageRank Value\n",
            "-----------------------------------\n",
            "  A   |     0.039\n",
            "  B   |     0.067\n",
            "  C   |     0.039\n",
            "  D   |     0.096\n",
            "  E   |     0.039\n",
            "  F   |     0.096\n",
            "  G   |     0.096\n",
            "  H   |     0.259\n",
            "  I   |     0.230\n",
            "  J   |     0.039\n",
            "-----------------------------------\n",
            "\n",
            "Highest PageRank after second iteration:\n",
            "Page H with PageRank value of 0.259\n"
          ]
        }
      ],
      "source": [
        "def calculate_new_pagerank(current_ranks, links):\n",
        "    damping = 0.85\n",
        "    base_rank = (1 - damping) / len(current_ranks)\n",
        "    new_ranks = {}\n",
        "    for page in current_ranks:\n",
        "        incoming_links = [p for p, outgoing in links.items() if page in outgoing]\n",
        "        rank_sum = 0\n",
        "        for source_page in incoming_links:\n",
        "            num_outgoing = len(links[source_page])\n",
        "            if num_outgoing > 0:\n",
        "                rank_sum += current_ranks[source_page] / num_outgoing\n",
        "        new_ranks[page] = base_rank + damping * rank_sum\n",
        "    total_rank = sum(new_ranks.values())\n",
        "    new_ranks = {page: rank / total_rank for page, rank in new_ranks.items()}\n",
        "    return new_ranks\n",
        "\n",
        "def print_ranks(ranks, iteration):\n",
        "    print(f\"\\nPageRank values after iteration {iteration}:\")\n",
        "    print(\"-\" * 35)\n",
        "    print(\"Page  |  PageRank Value\")\n",
        "    print(\"-\" * 35)\n",
        "    for page, rank in ranks.items():\n",
        "        print(f\"  {page}   |     {rank:.3f}\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "# Define the web structure\n",
        "A = 'https://www.who.int/news-room/fact-sheets/detail/hiv-drug-resistance'\n",
        "B = 'https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance'\n",
        "C = 'https://www.who.int/publications/who-guidelines'\n",
        "D = 'https://www.who.int/publications/i/item/guidelines-for-malaria'\n",
        "E = 'https://www.who.int/news-room/fact-sheets/detail/multi-drug-resistant-gonorrhoea'\n",
        "F = 'https://www.who.int/news-room/fact-sheets/detail/antimicrobial-resistance'\n",
        "G = 'https://www.who.int/campaigns/world-mental-health-day'\n",
        "H = 'https://www.who.int/news-room/fact-sheets/detail/mental-health-strengthening-our-response'\n",
        "I = 'https://www.who.int/news-room/fact-sheets/detail/mental-health-in-emergencies'\n",
        "J = 'https://www.who.int/westernpacific/activities/promoting-mental-health'\n",
        "links = {\n",
        "    'A' : ['B', 'H'],\n",
        "    'C' : ['D'],\n",
        "    'E' : ['F'],\n",
        "    'G' : ['H', 'I'],\n",
        "    'J' : ['G']\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize PageRank values (1/10 for each page)\n",
        "pages = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
        "current_ranks = {page: 1/10 for page in pages}\n",
        "\n",
        "print(\"Web structure:\")\n",
        "for page, outlinks in links.items():\n",
        "    print(f\"Page {page} links to: {', '.join(outlinks)}\")\n",
        "\n",
        "# Print initial values\n",
        "print(\"\\nInitial PageRank Values:\")\n",
        "print_ranks(current_ranks, 0)\n",
        "\n",
        "for i in range(1, 3):\n",
        "    current_ranks = calculate_new_pagerank(current_ranks, links)\n",
        "    print_ranks(current_ranks, i)\n",
        "\n",
        "# Find highest PageRank after second iteration\n",
        "highest_page = max(current_ranks.items(), key=lambda x: x[1])\n",
        "print(f\"\\nHighest PageRank after second iteration:\")\n",
        "print(f\"Page {highest_page[0]} with PageRank value of {highest_page[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}